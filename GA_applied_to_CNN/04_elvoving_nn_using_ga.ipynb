{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import random, randint, seed\n",
    "from statistics import mean\n",
    "from copy import deepcopy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import copy\n",
    "\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import PIL.Image as Image\n",
    "from torchvision import datasets,models\n",
    "\n",
    "from sympy.solvers import solve\n",
    "from sympy import Symbol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dowload the dataset\n",
    "# prepare a preprocessing pipeline \n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,)),\n",
    "])\n",
    "\n",
    "# We download the train and the test dataset in the given root and applying the given transforms\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True,  download=True, transform=transform)\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False,  download=True, transform=transform)\n",
    "\n",
    "batch_size=4\n",
    "\n",
    "# dataloaders\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,  shuffle=True, num_workers=2)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,  shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, genotype):\n",
    "        super().__init__()\n",
    "        self.genotype = genotype\n",
    "        self.layerlist = []\n",
    "        self.input_channels = 1\n",
    "        self.output_channels = 6\n",
    "        self.output_size = 10\n",
    "        self.kernel_size = 5\n",
    "        self.input_size = 28\n",
    "        self.final_size = 7\n",
    "\n",
    "        self.build_layers(genotype)\n",
    "        self.layers = torch.nn.Sequential(*self.layerlist)        \n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layers(x)\n",
    "        return out\n",
    "    \n",
    "    def build_layers(self, genotype):\n",
    "        for i, gene in enumerate(genotype): \n",
    "            # add final layer\n",
    "            if gene == 0:\n",
    "                # target output has the same size finalxfinal for each network\n",
    "                self.layerlist.append(torch.nn.AdaptiveAvgPool2d(self.final_size))\n",
    "                self.layerlist.append(torch.nn.Flatten())\n",
    "                size = self.output_channels*(self.final_size)**2\n",
    "                self.layerlist.append(torch.nn.Linear(size, 120))\n",
    "                self.layerlist.append(torch.nn.ReLU())\n",
    "                self.layerlist.append(torch.nn.Linear(120, 84))\n",
    "                self.layerlist.append(torch.nn.ReLU())\n",
    "                self.layerlist.append(torch.nn.Linear(84, self.output_size))\n",
    "\n",
    "            # add another intermediate layer\n",
    "            elif gene == 1:\n",
    "                output_channels = np.random.randint(1, 10)\n",
    "                if genotype[i+1] == 2:\n",
    "                    output_channels = 4 # here to be set not locally\n",
    "                   \n",
    "                kernel_size = np.random.randint(2, 5)\n",
    "                padding = 2\n",
    "                self.layerlist.append(torch.nn.Conv2d(in_channels=self.input_channels, out_channels=output_channels, kernel_size=kernel_size, stride=1, padding=padding))\n",
    "                self.layerlist.append(torch.nn.ReLU())\n",
    "                self.layerlist.append(torch.nn.MaxPool2d(2, 2))\n",
    "                self.input_size = int((self.input_size - kernel_size + 1 + padding*2)/2)\n",
    "                self.output_channels = output_channels\n",
    "                self.input_channels = output_channels\n",
    "                self.kernel_size = kernel_size\n",
    "\n",
    "            # add non-parametrizable block\n",
    "            elif gene == 2:\n",
    "                output_channels = 6\n",
    "                kernel_size = 3\n",
    "                padding = 2\n",
    "\n",
    "                self.layerlist.append(torch.nn.Conv2d(in_channels=4, out_channels=output_channels, kernel_size=kernel_size, stride=1, padding=padding))\n",
    "                self.layerlist.append(torch.nn.ReLU())\n",
    "                self.layerlist.append(torch.nn.MaxPool2d(2, 2))\n",
    "                self.input_size = int((self.input_size - kernel_size + 1 + padding*2)/2)\n",
    "                self.output_channels = output_channels\n",
    "                self.input_channels = output_channels\n",
    "                self.kernel_size = kernel_size\n",
    "\n",
    "    # mate function combines two networks by randomly selecting weights from one of the parents\n",
    "    def mate(self, other, mutate=True):\n",
    "        child = copy.deepcopy(self)\n",
    "        i = 0\n",
    "        while i < len(child.genotype):#for i in range(len(child.genotype)): # [1,1,2,0]\n",
    "          # change only blocks marked as \n",
    "            if self.genotype[i]==1:\n",
    "                pass_on = np.random.rand(1) < 0.5\n",
    "                while self.genotype[i]==1:\n",
    "                    for j in range(3): # 3 layers per block\n",
    "                        child.layers[i+j] = self.layers[i+j] if pass_on else other.layers[i+j]\n",
    "                    i += 1\n",
    "\n",
    "            elif self.genotype[i]==2:\n",
    "                pass_on = np.random.rand(1) < 0.5\n",
    "                for j in range(3): # 3 layers per block\n",
    "                    child.layers[i+j] = self.layers[i+j] if pass_on else other.layers[i+j]\n",
    "                    \n",
    "                i += 1\n",
    "            \n",
    "            elif self.genotype[i]==0:\n",
    "                pass_on = np.random.rand(1) < 0.5\n",
    "                for j in range(7): # 3 layers per block\n",
    "                    child.layers[i+j] = self.layers[i+j] if pass_on else other.layers[i+j]\n",
    "                    \n",
    "                i += 1\n",
    "\n",
    "        if mutate:\n",
    "            child.mutate(stdev=np.random.rand(1)/10)\n",
    "        return child\n",
    "\n",
    "    # the mutation step is realized as the addition of Gaussian noise to each weight in the network\n",
    "    def mutate(self, stdev=0.03):\n",
    "        for i in range(len(self.layers)):\n",
    "            if isinstance(self.layers[i], nn.Conv2d) or isinstance(self.layers[i], nn.Linear):\n",
    "              with torch.no_grad():\n",
    "                self.layers[i].weight +=  torch.tensor(np.random.normal(0,stdev, list(self.layers[i].weight.shape)))\n",
    "                self.layers[i].bias +=  torch.tensor(np.random.normal(0, stdev, list(self.layers[i].bias.shape)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvNet(\n",
       "  (layers): Sequential(\n",
       "    (0): Conv2d(1, 4, kernel_size=(4, 4), stride=(1, 1), padding=(2, 2))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(4, 6, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
       "    (4): ReLU()\n",
       "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): AdaptiveAvgPool2d(output_size=7)\n",
       "    (7): Flatten(start_dim=1, end_dim=-1)\n",
       "    (8): Linear(in_features=294, out_features=120, bias=True)\n",
       "    (9): ReLU()\n",
       "    (10): Linear(in_features=120, out_features=84, bias=True)\n",
       "    (11): ReLU()\n",
       "    (12): Linear(in_features=84, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m1 = ConvNet([1,2,0])\n",
    "m2 = ConvNet([1,2,0])\n",
    "\n",
    "m1.mate(m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The net takes as input object with size:  torch.Size([4, 1, 28, 28])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [2, 1, 1, 28, 28]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/francesco/Repository/Genetics_ANN/GA_applied_to_CNN/04_elvoving_nn_using_ga.ipynb Cell 5\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/francesco/Repository/Genetics_ANN/GA_applied_to_CNN/04_elvoving_nn_using_ga.ipynb#X42sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m y \u001b[39m=\u001b[39m model(inputs)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/francesco/Repository/Genetics_ANN/GA_applied_to_CNN/04_elvoving_nn_using_ga.ipynb#X42sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mThe net takes as input object with size: \u001b[39m\u001b[39m\"\u001b[39m, inputs\u001b[39m.\u001b[39mshape)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/francesco/Repository/Genetics_ANN/GA_applied_to_CNN/04_elvoving_nn_using_ga.ipynb#X42sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m summary(model, input_size\u001b[39m=\u001b[39;49m(\u001b[39m1\u001b[39;49m, \u001b[39m1\u001b[39;49m, \u001b[39m28\u001b[39;49m, \u001b[39m28\u001b[39;49m))\n",
      "File \u001b[0;32m~/anaconda3/envs/deep_le/lib/python3.9/site-packages/torchsummary/torchsummary.py:72\u001b[0m, in \u001b[0;36msummary\u001b[0;34m(model, input_size, batch_size, device)\u001b[0m\n\u001b[1;32m     68\u001b[0m model\u001b[39m.\u001b[39mapply(register_hook)\n\u001b[1;32m     70\u001b[0m \u001b[39m# make a forward pass\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39m# print(x.shape)\u001b[39;00m\n\u001b[0;32m---> 72\u001b[0m model(\u001b[39m*\u001b[39;49mx)\n\u001b[1;32m     74\u001b[0m \u001b[39m# remove these hooks\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[39mfor\u001b[39;00m h \u001b[39min\u001b[39;00m hooks:\n",
      "File \u001b[0;32m~/anaconda3/envs/deep_le/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/home/francesco/Repository/Genetics_ANN/GA_applied_to_CNN/04_elvoving_nn_using_ga.ipynb Cell 5\u001b[0m in \u001b[0;36mConvNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/francesco/Repository/Genetics_ANN/GA_applied_to_CNN/04_elvoving_nn_using_ga.ipynb#X42sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/francesco/Repository/Genetics_ANN/GA_applied_to_CNN/04_elvoving_nn_using_ga.ipynb#X42sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayers(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/francesco/Repository/Genetics_ANN/GA_applied_to_CNN/04_elvoving_nn_using_ga.ipynb#X42sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/anaconda3/envs/deep_le/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/deep_le/lib/python3.9/site-packages/torch/nn/modules/container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    138\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 139\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    140\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/deep_le/lib/python3.9/site-packages/torch/nn/modules/module.py:1148\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     bw_hook \u001b[39m=\u001b[39m hooks\u001b[39m.\u001b[39mBackwardHook(\u001b[39mself\u001b[39m, full_backward_hooks)\n\u001b[1;32m   1146\u001b[0m     \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m bw_hook\u001b[39m.\u001b[39msetup_input_hook(\u001b[39minput\u001b[39m)\n\u001b[0;32m-> 1148\u001b[0m result \u001b[39m=\u001b[39m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1149\u001b[0m \u001b[39mif\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1150\u001b[0m     \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m (\u001b[39m*\u001b[39m_global_forward_hooks\u001b[39m.\u001b[39mvalues(), \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n",
      "File \u001b[0;32m~/anaconda3/envs/deep_le/lib/python3.9/site-packages/torch/nn/modules/conv.py:457\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 457\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/anaconda3/envs/deep_le/lib/python3.9/site-packages/torch/nn/modules/conv.py:453\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    450\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    451\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    452\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 453\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    454\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [2, 1, 1, 28, 28]"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123456)\n",
    "\n",
    "# genotype can be whatever you want but it must finish with [2,0]\n",
    "model=ConvNet([1,2,1,2,0])\n",
    "\n",
    "dataloader_iterator = iter(trainloader)\n",
    "inputs, labels = next(dataloader_iterator)\n",
    "y = model(inputs)\n",
    "print(\"The net takes as input object with size: \", inputs.shape)\n",
    "summary(model, input_size=(1, 1, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the network using SGD but without using all the training data at once\n",
    "def train(model, trainloader):\n",
    "    inspected = 50000\n",
    "    iterations = int(inspected / 4)\n",
    "\n",
    "    dataloader_iterator = iter(trainloader)\n",
    "\n",
    "    # define the loss function and the optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "    for i in range(iterations):\n",
    "        try:\n",
    "            inputs, labels = next(dataloader_iterator)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # calculate outputs by running images through the network\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        except StopIteration:\n",
    "            print(\"StopIteration, not enough data\")\n",
    "    \n",
    "    return model\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at global performances on the testset\n",
    "def eval(model):\n",
    "  correct = 0\n",
    "  total = 0\n",
    "  # since we're not training, we don't need to calculate the gradients for our outputs\n",
    "  with torch.no_grad():\n",
    "      for data in testloader:\n",
    "          images, labels = data\n",
    "          # calculate outputs by running images through the network\n",
    "          outputs = model.forward(images)\n",
    "          # the class with the highest energy is what we choose as prediction\n",
    "          _, predicted = torch.max(outputs.data, 1)\n",
    "          total += labels.size(0)\n",
    "          correct += (predicted == labels).sum().item()\n",
    "\n",
    "  accuracy = 100 * correct // total\n",
    "  print(f'Accuracy of the network on the 10000 test images: {accuracy} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 94 %\n"
     ]
    }
   ],
   "source": [
    "train(model, trainloader)\n",
    "eval(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class evolution():\n",
    "    def __init__(self, fun, population_size=10, holdout=1, mating=True):\n",
    "        \"\"\"\n",
    "        initial function fun is a function to produce nets, used for the original population\n",
    "        scoring_function must be a function which accepts a net as input and returns a float\n",
    "        \"\"\"\n",
    "        self.population_size = population_size\n",
    "        self.population = []\n",
    "        for _ in range(population_size):\n",
    "            torch.manual_seed(torch.randint(0, 100000, (1,)))\n",
    "            self.population.append(fun([1,2,0]))\n",
    "\n",
    "        self.best_organism = self.population[-1]\n",
    "        self.best_score = self.scoring_function(self.best_organism)\n",
    "\n",
    "        self.holdout = max(1, int(holdout * population_size))\n",
    "\n",
    "        self.mating = True\n",
    "        \n",
    "\n",
    "    def generation(self):\n",
    "        self.population = [train(self.population[0], trainloader) for x in self.population]\n",
    "        scores = [self.scoring_function(x) for x in self.population]\n",
    "        self.population = [self.population[x] for x in np.argsort(scores)[::-1]]\n",
    "\n",
    "        # update best organism and respective accuracy\n",
    "        self.best_organism = copy.deepcopy(self.population[0])\n",
    "        self.best_score = sorted(scores)[-1]\n",
    "        \n",
    "        new_population = [self.best_organism] # Ensure best organism survives\n",
    "        for i in range(self.population_size - 1):\n",
    "            parent_1_idx = i % self.holdout\n",
    "            if self.mating:\n",
    "                parent_2_idx = min(self.population_size - 1, int(np.random.exponential(self.holdout)))\n",
    "            else:\n",
    "                parent_2_idx = parent_1_idx\n",
    "            offspring = self.population[parent_1_idx].mate(self.population[parent_2_idx]) # to build new generation functions mate and mutate are called as well\n",
    "            new_population.append(offspring)\n",
    "        \n",
    "        self.population = new_population\n",
    "\n",
    "    def get_best_organism(self, repeats=1):        \n",
    "        return self.best_organism, self.best_score\n",
    "\n",
    "    def scoring_function(self, model):\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        # since we're not training, we don't need to calculate the gradients for our outputs\n",
    "        with torch.no_grad():\n",
    "            for data in testloader:\n",
    "                images, labels = data\n",
    "                # calculate outputs by running images through the network\n",
    "                outputs = model.forward(images)\n",
    "                # the class with the highest energy is what we choose as prediction\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                \n",
    "\n",
    "        accuracy = 100 * correct // total\n",
    "        return(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function to create the initial population\n",
    "net_creator = lambda g : ConvNet(g)\n",
    "\n",
    "# initialize the class which will handle evolution of the NNs\n",
    "curr_env = evolution(net_creator, population_size=4, holdout=0.6, mating=True)\n",
    "\n",
    "# get current most suitable network (organism)\n",
    "best_net, score = curr_env.get_best_organism()\n",
    "acc = [score]\n",
    "best_nets = [best_net]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "├─Sequential: 1-1                        --\n",
      "|    └─Conv2d: 2-1                       20\n",
      "|    └─ReLU: 2-2                         --\n",
      "|    └─MaxPool2d: 2-3                    --\n",
      "|    └─Conv2d: 2-4                       222\n",
      "|    └─ReLU: 2-5                         --\n",
      "|    └─MaxPool2d: 2-6                    --\n",
      "|    └─Conv2d: 2-7                       55\n",
      "|    └─ReLU: 2-8                         --\n",
      "|    └─MaxPool2d: 2-9                    --\n",
      "|    └─Flatten: 2-10                     --\n",
      "|    └─Linear: 2-11                      3,120\n",
      "|    └─ReLU: 2-12                        --\n",
      "|    └─Linear: 2-13                      10,164\n",
      "|    └─ReLU: 2-14                        --\n",
      "|    └─Linear: 2-15                      850\n",
      "=================================================================\n",
      "Total params: 14,431\n",
      "Trainable params: 14,431\n",
      "Non-trainable params: 0\n",
      "=================================================================\n",
      "Net  0 :\n",
      " =================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "├─Sequential: 1-1                        --\n",
      "|    └─Conv2d: 2-1                       20\n",
      "|    └─ReLU: 2-2                         --\n",
      "|    └─MaxPool2d: 2-3                    --\n",
      "|    └─Conv2d: 2-4                       222\n",
      "|    └─ReLU: 2-5                         --\n",
      "|    └─MaxPool2d: 2-6                    --\n",
      "|    └─Conv2d: 2-7                       55\n",
      "|    └─ReLU: 2-8                         --\n",
      "|    └─MaxPool2d: 2-9                    --\n",
      "|    └─Flatten: 2-10                     --\n",
      "|    └─Linear: 2-11                      3,120\n",
      "|    └─ReLU: 2-12                        --\n",
      "|    └─Linear: 2-13                      10,164\n",
      "|    └─ReLU: 2-14                        --\n",
      "|    └─Linear: 2-15                      850\n",
      "=================================================================\n",
      "Total params: 14,431\n",
      "Trainable params: 14,431\n",
      "Non-trainable params: 0\n",
      "=================================================================\n",
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "├─Sequential: 1-1                        --\n",
      "|    └─Conv2d: 2-1                       20\n",
      "|    └─ReLU: 2-2                         --\n",
      "|    └─MaxPool2d: 2-3                    --\n",
      "|    └─Conv2d: 2-4                       222\n",
      "|    └─ReLU: 2-5                         --\n",
      "|    └─MaxPool2d: 2-6                    --\n",
      "|    └─Conv2d: 2-7                       225\n",
      "|    └─ReLU: 2-8                         --\n",
      "|    └─MaxPool2d: 2-9                    --\n",
      "|    └─Flatten: 2-10                     --\n",
      "|    └─Linear: 2-11                      27,120\n",
      "|    └─ReLU: 2-12                        --\n",
      "|    └─Linear: 2-13                      10,164\n",
      "|    └─ReLU: 2-14                        --\n",
      "|    └─Linear: 2-15                      850\n",
      "=================================================================\n",
      "Total params: 38,601\n",
      "Trainable params: 38,601\n",
      "Non-trainable params: 0\n",
      "=================================================================\n",
      "Net  1 :\n",
      " =================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "├─Sequential: 1-1                        --\n",
      "|    └─Conv2d: 2-1                       20\n",
      "|    └─ReLU: 2-2                         --\n",
      "|    └─MaxPool2d: 2-3                    --\n",
      "|    └─Conv2d: 2-4                       222\n",
      "|    └─ReLU: 2-5                         --\n",
      "|    └─MaxPool2d: 2-6                    --\n",
      "|    └─Conv2d: 2-7                       225\n",
      "|    └─ReLU: 2-8                         --\n",
      "|    └─MaxPool2d: 2-9                    --\n",
      "|    └─Flatten: 2-10                     --\n",
      "|    └─Linear: 2-11                      27,120\n",
      "|    └─ReLU: 2-12                        --\n",
      "|    └─Linear: 2-13                      10,164\n",
      "|    └─ReLU: 2-14                        --\n",
      "|    └─Linear: 2-15                      850\n",
      "=================================================================\n",
      "Total params: 38,601\n",
      "Trainable params: 38,601\n",
      "Non-trainable params: 0\n",
      "=================================================================\n",
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "├─Sequential: 1-1                        --\n",
      "|    └─Conv2d: 2-1                       20\n",
      "|    └─ReLU: 2-2                         --\n",
      "|    └─MaxPool2d: 2-3                    --\n",
      "|    └─Conv2d: 2-4                       222\n",
      "|    └─ReLU: 2-5                         --\n",
      "|    └─MaxPool2d: 2-6                    --\n",
      "|    └─Conv2d: 2-7                       100\n",
      "|    └─ReLU: 2-8                         --\n",
      "|    └─MaxPool2d: 2-9                    --\n",
      "|    └─Flatten: 2-10                     --\n",
      "|    └─Linear: 2-11                      12,120\n",
      "|    └─ReLU: 2-12                        --\n",
      "|    └─Linear: 2-13                      10,164\n",
      "|    └─ReLU: 2-14                        --\n",
      "|    └─Linear: 2-15                      850\n",
      "=================================================================\n",
      "Total params: 23,476\n",
      "Trainable params: 23,476\n",
      "Non-trainable params: 0\n",
      "=================================================================\n",
      "Net  2 :\n",
      " =================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "├─Sequential: 1-1                        --\n",
      "|    └─Conv2d: 2-1                       20\n",
      "|    └─ReLU: 2-2                         --\n",
      "|    └─MaxPool2d: 2-3                    --\n",
      "|    └─Conv2d: 2-4                       222\n",
      "|    └─ReLU: 2-5                         --\n",
      "|    └─MaxPool2d: 2-6                    --\n",
      "|    └─Conv2d: 2-7                       100\n",
      "|    └─ReLU: 2-8                         --\n",
      "|    └─MaxPool2d: 2-9                    --\n",
      "|    └─Flatten: 2-10                     --\n",
      "|    └─Linear: 2-11                      12,120\n",
      "|    └─ReLU: 2-12                        --\n",
      "|    └─Linear: 2-13                      10,164\n",
      "|    └─ReLU: 2-14                        --\n",
      "|    └─Linear: 2-15                      850\n",
      "=================================================================\n",
      "Total params: 23,476\n",
      "Trainable params: 23,476\n",
      "Non-trainable params: 0\n",
      "=================================================================\n",
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "├─Sequential: 1-1                        --\n",
      "|    └─Conv2d: 2-1                       40\n",
      "|    └─ReLU: 2-2                         --\n",
      "|    └─MaxPool2d: 2-3                    --\n",
      "|    └─Conv2d: 2-4                       222\n",
      "|    └─ReLU: 2-5                         --\n",
      "|    └─MaxPool2d: 2-6                    --\n",
      "|    └─Conv2d: 2-7                       679\n",
      "|    └─ReLU: 2-8                         --\n",
      "|    └─MaxPool2d: 2-9                    --\n",
      "|    └─Flatten: 2-10                     --\n",
      "|    └─Linear: 2-11                      13,560\n",
      "|    └─ReLU: 2-12                        --\n",
      "|    └─Linear: 2-13                      10,164\n",
      "|    └─ReLU: 2-14                        --\n",
      "|    └─Linear: 2-15                      850\n",
      "=================================================================\n",
      "Total params: 25,515\n",
      "Trainable params: 25,515\n",
      "Non-trainable params: 0\n",
      "=================================================================\n",
      "Net  3 :\n",
      " =================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "├─Sequential: 1-1                        --\n",
      "|    └─Conv2d: 2-1                       40\n",
      "|    └─ReLU: 2-2                         --\n",
      "|    └─MaxPool2d: 2-3                    --\n",
      "|    └─Conv2d: 2-4                       222\n",
      "|    └─ReLU: 2-5                         --\n",
      "|    └─MaxPool2d: 2-6                    --\n",
      "|    └─Conv2d: 2-7                       679\n",
      "|    └─ReLU: 2-8                         --\n",
      "|    └─MaxPool2d: 2-9                    --\n",
      "|    └─Flatten: 2-10                     --\n",
      "|    └─Linear: 2-11                      13,560\n",
      "|    └─ReLU: 2-12                        --\n",
      "|    └─Linear: 2-13                      10,164\n",
      "|    └─ReLU: 2-14                        --\n",
      "|    └─Linear: 2-15                      850\n",
      "=================================================================\n",
      "Total params: 25,515\n",
      "Trainable params: 25,515\n",
      "Non-trainable params: 0\n",
      "=================================================================\n"
     ]
    }
   ],
   "source": [
    "# check different architectures\n",
    "for i,net in enumerate(curr_env.population):\n",
    "    print(\"Net \", i, \":\\n\",summary(net, input_size=(1, 1, 28, 28)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation  0 's best network accuracy:  33 %\n",
      "Generation  1 's best network accuracy:  71 %\n",
      "Generation  2 's best network accuracy:  81 %\n",
      "Generation  3 's best network accuracy:  88 %\n",
      "Generation  4 's best network accuracy:  91 %\n"
     ]
    }
   ],
   "source": [
    "generations = 5\n",
    "for i in range(generations):\n",
    "    curr_env.generation()\n",
    "    this_generation_best, score = curr_env.get_best_organism()\n",
    "    best_nets.append(this_generation_best)\n",
    "    acc.append(score)\n",
    "    #if i%5==0:\n",
    "    print(\"Generation \", i , \"'s best network accuracy: \", score, \"%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('deep_le')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a142b7d21e8575b7566c168744f24153333bb674c4e2523209192565d5391819"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
