{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import random, randint, seed\n",
    "from statistics import mean\n",
    "from copy import deepcopy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import copy\n",
    "\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import PIL.Image as Image\n",
    "from torchvision import datasets,models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dowload the dataset\n",
    "# prepare a preprocessing pipeline \n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,)),\n",
    "])\n",
    "\n",
    "# We download the train and the test dataset in the given root and applying the given transforms\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True,  download=True, transform=transform)\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False,  download=True, transform=transform)\n",
    "\n",
    "batch_size=4\n",
    "\n",
    "# dataloaders\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,  shuffle=True, num_workers=2)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,  shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, genotype):\n",
    "        super().__init__()\n",
    "        self.layerlist = []\n",
    "        self.input_channels = 1\n",
    "        self.output_channels = 6\n",
    "        self.output_size = 10\n",
    "        self.kernel_size = 5\n",
    "        self.input_size = 28\n",
    "\n",
    "        self.build_layers(genotype)\n",
    "        self.layers = torch.nn.Sequential(*self.layerlist)        \n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layers(x)\n",
    "        return out\n",
    "    \n",
    "    def build_layers(self, genotype):\n",
    "        for i, gene in enumerate(genotype): \n",
    "            # add final layer\n",
    "            if gene == 0:\n",
    "                self.layerlist.append(torch.nn.Flatten())\n",
    "                size = self.output_channels*(self.input_size)**2\n",
    "                self.layerlist.append(torch.nn.Linear(size, 120))\n",
    "                self.layerlist.append(torch.nn.ReLU())\n",
    "                self.layerlist.append(torch.nn.Linear(120, 84))\n",
    "                self.layerlist.append(torch.nn.ReLU())\n",
    "                self.layerlist.append(torch.nn.Linear(84, self.output_size))\n",
    "\n",
    "            # add another intermediate layer\n",
    "            elif gene == 1:\n",
    "                output_channels = np.random.randint(1, 10)\n",
    "                if genotype[i+1] == 2:\n",
    "                    output_channels = 4\n",
    "                kernel_size = np.random.randint(2, 5)\n",
    "                self.layerlist.append(torch.nn.Conv2d(in_channels=self.input_channels, out_channels=output_channels, kernel_size=kernel_size, stride=1, padding=2))\n",
    "                self.layerlist.append(torch.nn.ReLU())\n",
    "                self.layerlist.append(torch.nn.MaxPool2d(2, 2))\n",
    "                self.input_size = int((self.input_size - kernel_size + 1 + 4)/2)\n",
    "                self.output_channels = output_channels\n",
    "                self.input_channels = output_channels\n",
    "                self.kernel_size = kernel_size\n",
    "\n",
    "            # add non-parametrizable block\n",
    "            elif gene == 2:\n",
    "                output_channels = 6\n",
    "                kernel_size = 3\n",
    "                self.layerlist.append(torch.nn.Conv2d(in_channels=4, out_channels=output_channels, kernel_size=kernel_size,stride=1, padding=2))\n",
    "                self.layerlist.append(torch.nn.ReLU())\n",
    "                self.layerlist.append(torch.nn.MaxPool2d(2, 2))\n",
    "                self.input_size = int((self.input_size - kernel_size + 1 + 4)/2)\n",
    "                self.output_channels = output_channels\n",
    "                self.input_channels = output_channels\n",
    "                self.kernel_size = kernel_size\n",
    "\n",
    "    # mate function combines two networks by randomly selecting weights from one of the parents\n",
    "    def mate(self, other, mutate=True):\n",
    "        child = copy.deepcopy(self)\n",
    "        for i in range(len(child.layers)):\n",
    "          # change only convolutional and linear layers\n",
    "          if isinstance(child.layers[i], nn.Linear) or isinstance(child.layers[i], nn.Conv2d):\n",
    "            pass_on = np.random.rand(1) < 0.5\n",
    "            child.layers[i] = self.layers[i] if pass_on else other.layers[i]\n",
    "        if mutate:\n",
    "            child.mutate(stdev=np.random.rand(1)/10)\n",
    "        return child\n",
    "\n",
    "    # the mutation step is realized as the addition of Gaussian noise to each weight in the network\n",
    "    def mutate(self, stdev=0.03):\n",
    "        for i in range(len(self.layers)):\n",
    "            if isinstance(self.layers[i], nn.Conv2d) or isinstance(self.layers[i], nn.Linear):\n",
    "              with torch.no_grad():\n",
    "                self.layers[i].weight +=  torch.tensor(np.random.normal(0,stdev, list(self.layers[i].weight.shape)))\n",
    "                self.layers[i].bias +=  torch.tensor(np.random.normal(0, stdev, list(self.layers[i].bias.shape)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1, 28, 28])\n",
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "├─Sequential: 1-1                        --\n",
      "|    └─Conv2d: 2-1                       40\n",
      "|    └─ReLU: 2-2                         --\n",
      "|    └─MaxPool2d: 2-3                    --\n",
      "|    └─Conv2d: 2-4                       222\n",
      "|    └─ReLU: 2-5                         --\n",
      "|    └─MaxPool2d: 2-6                    --\n",
      "|    └─Conv2d: 2-7                       679\n",
      "|    └─ReLU: 2-8                         --\n",
      "|    └─MaxPool2d: 2-9                    --\n",
      "|    └─Flatten: 2-10                     --\n",
      "|    └─Linear: 2-11                      13,560\n",
      "|    └─ReLU: 2-12                        --\n",
      "|    └─Linear: 2-13                      10,164\n",
      "|    └─ReLU: 2-14                        --\n",
      "|    └─Linear: 2-15                      850\n",
      "=================================================================\n",
      "Total params: 25,515\n",
      "Trainable params: 25,515\n",
      "Non-trainable params: 0\n",
      "=================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "├─Sequential: 1-1                        --\n",
       "|    └─Conv2d: 2-1                       40\n",
       "|    └─ReLU: 2-2                         --\n",
       "|    └─MaxPool2d: 2-3                    --\n",
       "|    └─Conv2d: 2-4                       222\n",
       "|    └─ReLU: 2-5                         --\n",
       "|    └─MaxPool2d: 2-6                    --\n",
       "|    └─Conv2d: 2-7                       679\n",
       "|    └─ReLU: 2-8                         --\n",
       "|    └─MaxPool2d: 2-9                    --\n",
       "|    └─Flatten: 2-10                     --\n",
       "|    └─Linear: 2-11                      13,560\n",
       "|    └─ReLU: 2-12                        --\n",
       "|    └─Linear: 2-13                      10,164\n",
       "|    └─ReLU: 2-14                        --\n",
       "|    └─Linear: 2-15                      850\n",
       "=================================================================\n",
       "Total params: 25,515\n",
       "Trainable params: 25,515\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123456)\n",
    "\n",
    "model=ConvNet([1,2,1,0])\n",
    "# feed data into the model\n",
    "#x = trainloader.dataset.data[0].unsqueeze(0).float()\n",
    "#y = model.forward(x)\n",
    "\n",
    "dataloader_iterator = iter(trainloader)\n",
    "inputs, labels = next(dataloader_iterator)\n",
    "#y = model(inputs)\n",
    "print(inputs.shape)\n",
    "\n",
    "summary(model, input_size=(1, 1, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the network using SGD but without using all the training data at once\n",
    "def train(model, trainloader):\n",
    "    inspected = 10000\n",
    "    iterations = int(inspected / 4)\n",
    "\n",
    "    dataloader_iterator = iter(trainloader)\n",
    "\n",
    "    # define the loss function and the optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "    for i in range(iterations):\n",
    "        try:\n",
    "            inputs, labels = next(dataloader_iterator)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # calculate outputs by running images through the network\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        except StopIteration:\n",
    "            print(\"StopIteration, not enough data\")\n",
    "    \n",
    "    return model\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at global performances on the testset\n",
    "def eval(model):\n",
    "  correct = 0\n",
    "  total = 0\n",
    "  # since we're not training, we don't need to calculate the gradients for our outputs\n",
    "  with torch.no_grad():\n",
    "      for data in testloader:\n",
    "          images, labels = data\n",
    "          # calculate outputs by running images through the network\n",
    "          outputs = model.forward(images)\n",
    "          # the class with the highest energy is what we choose as prediction\n",
    "          _, predicted = torch.max(outputs.data, 1)\n",
    "          total += labels.size(0)\n",
    "          correct += (predicted == labels).sum().item()\n",
    "\n",
    "  accuracy = 100 * correct // total\n",
    "  print(f'Accuracy of the network on the 10000 test images: {accuracy} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 84 %\n"
     ]
    }
   ],
   "source": [
    "train(model, trainloader)\n",
    "eval(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class evolution():\n",
    "    def __init__(self, fun, population_size=10, holdout=1, mating=True):\n",
    "        \"\"\"\n",
    "        initial function fun is a function to produce nets, used for the original population\n",
    "        scoring_function must be a function which accepts a net as input and returns a float\n",
    "        \"\"\"\n",
    "        self.population_size = population_size\n",
    "        self.population = [fun([1,2,1,0]) for _ in range(population_size)]\n",
    "        self.best_organism = self.population[-1]\n",
    "        self.best_score = self.scoring_function(self.best_organism)\n",
    "\n",
    "        self.holdout = max(1, int(holdout * population_size))\n",
    "\n",
    "        self.mating = True\n",
    "        \n",
    "\n",
    "    def generation(self):\n",
    "        self.population = [train(self.population[0], trainloader) for x in self.population]\n",
    "        scores = [self.scoring_function(x) for x in self.population]\n",
    "        self.population = [self.population[x] for x in np.argsort(scores)[::-1]]\n",
    "\n",
    "        # update best organism and respective accuracy\n",
    "        self.best_organism = copy.deepcopy(self.population[0])\n",
    "        self.best_score = sorted(scores)[-1]\n",
    "        \n",
    "        new_population = [self.best_organism] # Ensure best organism survives\n",
    "        for i in range(self.population_size - 1):\n",
    "            parent_1_idx = i % self.holdout\n",
    "            if self.mating:\n",
    "                parent_2_idx = min(self.population_size - 1, int(np.random.exponential(self.holdout)))\n",
    "            else:\n",
    "                parent_2_idx = parent_1_idx\n",
    "            offspring = self.population[parent_1_idx].mate(self.population[parent_2_idx]) # to build new generation functions mate and mutate are called as well\n",
    "            new_population.append(offspring)\n",
    "        \n",
    "        self.population = new_population\n",
    "\n",
    "    def get_best_organism(self, repeats=1):        \n",
    "        return self.best_organism, self.best_score\n",
    "\n",
    "    def scoring_function(self, model):\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        # take each time a certain number of random images on which evaluating the network\n",
    "        inspected = 10000\n",
    "        iterations = int(inspected / 4)\n",
    "\n",
    "        dataloader_iterator = iter(testloader)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i in range(iterations):\n",
    "                try:\n",
    "                    images, labels = next(dataloader_iterator)\n",
    "\n",
    "                    # calculate outputs by running images through the network\n",
    "                    outputs = model.forward(images)\n",
    "                    # the class with the highest energy is what we choose as prediction\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    correct += (predicted == labels).sum().item()\n",
    "                    total += labels.size(0)\n",
    "                except StopIteration:\n",
    "                    print(\"StopIteration, not enough data\")\n",
    "                \n",
    "\n",
    "        accuracy = 100 * correct // total\n",
    "        return(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function to create the initial population\n",
    "net_creator = lambda g : ConvNet(g)\n",
    "\n",
    "# initialize the class which will handle evolution of the NNs\n",
    "curr_env = evolution(net_creator, population_size=4, holdout=0.6, mating=True)\n",
    "\n",
    "# get current most suitable network (organism)\n",
    "best_net, score = curr_env.get_best_organism()\n",
    "acc = [score]\n",
    "best_nets = [best_net]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[11]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "curr_env.population[0]\n",
    "dataloader_iterator = iter(trainloader)\n",
    "inputs, labels = next(dataloader_iterator)\n",
    "y = curr_env.population[0](inputs)\n",
    "print(inputs.shape)\n",
    "#summary(curr_env.population[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvNet(\n",
       "  (layers): Sequential(\n",
       "    (0): Conv2d(1, 4, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(4, 6, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
       "    (4): ReLU()\n",
       "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(6, 6, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
       "    (7): ReLU()\n",
       "    (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (9): Flatten(start_dim=1, end_dim=-1)\n",
       "    (10): Linear(in_features=150, out_features=120, bias=True)\n",
       "    (11): ReLU()\n",
       "    (12): Linear(in_features=120, out_features=84, bias=True)\n",
       "    (13): ReLU()\n",
       "    (14): Linear(in_features=84, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(curr_env.population[0], trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation  0 's best network accuracy:  90 %\n",
      "Generation  1 's best network accuracy:  97 %\n",
      "Generation  2 's best network accuracy:  96 %\n"
     ]
    }
   ],
   "source": [
    "generations = 3\n",
    "for i in range(generations):\n",
    "    curr_env.generation()\n",
    "    this_generation_best, score = curr_env.get_best_organism()\n",
    "    best_nets.append(this_generation_best)\n",
    "    acc.append(score)\n",
    "    #if i%5==0:\n",
    "    print(\"Generation \", i , \"'s best network accuracy: \", score, \"%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 ('deep-learn')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1d4df73e08e394aceedb93fd289f1fdb03087f2b8b3b3a9e1d14c5267e8bf280"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
