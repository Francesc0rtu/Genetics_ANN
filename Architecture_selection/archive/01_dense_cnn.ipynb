{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of a dense CNN for the CIFAR-10 dataset\n",
    "This is an implementation of a dense CNN for the CIFAR-10 dataset. The model is based on the paper [Densely Connected Convolutional Networks](https://arxiv.org/abs/1608.06993) by Gao Huang, Zhuang Liu, Laurens van der Maaten, Kilian Q. Weinberger. \n",
    "\n",
    "This will be the starting point to modify the structure of the network to test different architectures.\n",
    "\n",
    "The implementation is basend on [this](https://amaarora.github.io/2020/08/02/densenets.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import copy\n",
    "from collections import OrderedDict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _Transition(nn.Sequential):\n",
    "    def __init__(self, num_input_features, num_output_features):\n",
    "        super(_Transition, self).__init__()\n",
    "        self.add_module('BN', nn.BatchNorm2d(num_input_features))\n",
    "        self.add_module('1x1', nn.Conv2d(num_input_features, num_output_features,\n",
    "                                          kernel_size=1, stride=1, bias=False))\n",
    "        self.add_module('relu', nn.ReLU(inplace=True))\n",
    "        self.add_module('pool', nn.AvgPool2d(kernel_size=2, stride=2))\n",
    "\n",
    "\n",
    "#define dense layer and bottleneck layer\n",
    "class _DenseLayer(nn.Module):\n",
    "    def __init__(self, num_input_features, growth_rate, bn_size, drop_rate, memory_efficient=False):\n",
    "        super(_DenseLayer, self).__init__()\n",
    "        self.add_module('BN_1', nn.BatchNorm2d(num_input_features))\n",
    "        self.add_module('conv1', nn.Conv2d(num_input_features, bn_size *\n",
    "                                           growth_rate, kernel_size=1, stride=1,\n",
    "                                           bias=False))\n",
    "        self.add_module('relu_1', nn.ReLU(inplace=True))\n",
    "        self.add_module('BN_2', nn.BatchNorm2d(bn_size * growth_rate))\n",
    "        self.add_module('conv3', nn.Conv2d(bn_size * growth_rate, growth_rate,\n",
    "                                           kernel_size=5, padding=2, dilation=1, groups=2,\n",
    "                                           bias=False)) # questo Ã¨ il cromosoma\n",
    "\n",
    "       \n",
    "\n",
    "        self.add_module('relu_2', nn.ReLU(inplace=True))\n",
    "        self.drop_rate = float(drop_rate)\n",
    "        self.memory_efficient = memory_efficient\n",
    "\n",
    "    def bn_function(self, inputs):\n",
    "        \"Bottleneck function\"\n",
    "        # type: (List[Tensor]) -> Tensor\n",
    "        \n",
    "        concated_features = torch.cat(inputs, 1)\n",
    "        bottleneck_output = self.relu_1(self.conv1(self.BN_1(concated_features)))  # noqa: T484\n",
    "        \n",
    "        return bottleneck_output\n",
    "\n",
    "    def forward(self, input):  # noqa: F811\n",
    "        if torch.is_tensor(input):\n",
    "            prev_features = [input]\n",
    "        else:\n",
    "            prev_features = input\n",
    "        bottleneck_output = self.bn_function(prev_features)\n",
    "        new_features = self.relu_2(self.conv3(self.BN_2(bottleneck_output)))\n",
    "        if self.drop_rate > 0:\n",
    "            new_features = F.dropout(new_features, p=self.drop_rate,\n",
    "                                     training=self.training)\n",
    "        return new_features\n",
    "\n",
    "\n",
    "\n",
    "class _DenseBlock(nn.ModuleDict):\n",
    "    _version = 2\n",
    "\n",
    "    def __init__(self, num_layers, num_input_features, bn_size, growth_rate, drop_rate, memory_efficient=False):\n",
    "        super(_DenseBlock, self).__init__()\n",
    "        for i in range(num_layers):\n",
    "            layer = _DenseLayer(\n",
    "                num_input_features + i * growth_rate,\n",
    "                growth_rate=growth_rate,\n",
    "                bn_size=bn_size,\n",
    "                drop_rate=drop_rate,\n",
    "                memory_efficient=memory_efficient,\n",
    "            )\n",
    "            self.add_module('denselayer%d' % (i + 1), layer)\n",
    "\n",
    "    def forward(self, init_features):\n",
    "        features = [init_features]\n",
    "        for name, layer in self.items():\n",
    "            new_features = layer(features)\n",
    "            features.append(new_features)\n",
    "\n",
    "        \n",
    "        return torch.cat(features, 1)\n",
    "\n",
    "\n",
    "class DenseNet(nn.Module):\n",
    "    def __init__(self, growth_rate=2, block_config=(3, 3),\n",
    "                 num_init_features=3, bn_size=2, drop_rate=0, num_classes=10, memory_efficient=False):\n",
    "\n",
    "        super(DenseNet, self).__init__()\n",
    "\n",
    "        #Convolution and pooling part from table-1\n",
    "        # self.features = nn.Sequential(OrderedDict([\n",
    "        #     #('conv0', nn.Conv2d(3, num_init_features, kernel_size=7, stride=2,\n",
    "        #                        # padding=3, bias=False)),\n",
    "        #     ('norm0', nn.BatchNorm2d(num_init_features)),\n",
    "        #     ('relu0', nn.ReLU(inplace=True)),\n",
    "        #     ('pool0', nn.MaxPool2d(kernel_size=3, stride=2, padding=1)),\n",
    "        # ]))\n",
    "        # print(\"features:\", self.features)\n",
    "        self.features = nn.Sequential()\n",
    "     \n",
    "\n",
    "        num_features = num_init_features\n",
    "        for i, num_layers in enumerate(block_config):\n",
    "            block = _DenseBlock(num_layers=num_layers, num_input_features=num_features, bn_size=bn_size, growth_rate=growth_rate, drop_rate=drop_rate, memory_efficient=memory_efficient)\n",
    "            self.features.add_module('denseblock%d' % (i + 1), block)\n",
    "            num_features = num_features + num_layers * growth_rate\n",
    "            if i != len(block_config) - 1:\n",
    "                # add transition layer between denseblocks to \n",
    "                # downsample\n",
    "                trans = _Transition(num_input_features=num_features,\n",
    "                                    num_output_features=num_features // 2)\n",
    "                self.features.add_module('transition%d' % (i + 1), trans)\n",
    "                num_features = num_features // 2\n",
    "\n",
    "        # Final batch norm\n",
    "        self.features.add_module('norm5', nn.BatchNorm2d(num_features))\n",
    "\n",
    "        # Linear layer\n",
    "        self.classifier = nn.Linear(num_features, num_classes)\n",
    "       \n",
    "        # Official init from torch repo.\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.features(x)\n",
    "        out = F.relu(features, inplace=True)\n",
    "        out = F.adaptive_avg_pool2d(out, (1, 1))\n",
    "        out = torch.flatten(out, 1)\n",
    "        out = self.classifier(out)\n",
    "        return out\n",
    "\n",
    "net = DenseNet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application of the net in the cifar-10 dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fristly, import the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from scripts import cifar10\n",
    "trainloader, testloader, classes = cifar10.cifar10()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "net.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "print(torch.device('cuda:0' if torch.cuda.is_available() else 'cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING - loss 2.284794807434082 - performance 0.25\n",
      "TRAINING - loss 2.3153443430910015 - performance 0.08663366336633663\n",
      "TRAINING - loss 2.3054840327495367 - performance 0.08333333333333333\n",
      "TRAINING - loss 2.298742900259075 - performance 0.09053156146179402\n",
      "TRAINING - loss 2.2900160286491946 - performance 0.10224438902743142\n",
      "TRAINING - loss 2.286401783872745 - performance 0.10828343313373254\n",
      "TRAINING - loss 2.2802732216934993 - performance 0.12104825291181365\n",
      "TRAINING - loss 2.280158241533179 - performance 0.12375178316690442\n",
      "TRAINING - loss 2.277725527200211 - performance 0.12827715355805244\n",
      "TRAINING - loss 2.275012868622961 - performance 0.13124306326304105\n",
      "TRAINING - loss 2.272344369631071 - performance 0.13236763236763235\n",
      "TRAINING - loss 2.2717197907176696 - performance 0.13328792007266121\n",
      "TRAINING - loss 2.267995591763156 - performance 0.13905079100749376\n",
      "TRAINING - loss 2.2655475667400786 - performance 0.13931591083781705\n",
      "TRAINING - loss 2.2636991913704936 - performance 0.13900785153461814\n",
      "TRAINING - loss 2.2596779119801633 - performance 0.14706862091938708\n",
      "TRAINING - loss 2.2566200982474447 - performance 0.14803247970018737\n",
      "TRAINING - loss 2.253275565579665 - performance 0.15020576131687244\n",
      "TRAINING - loss 2.249736958579975 - performance 0.1521377012770683\n",
      "TRAINING - loss 2.2457042623356354 - performance 0.15583903208837455\n",
      "TRAINING - loss 2.2427242871584268 - performance 0.1570464767616192\n",
      "TRAINING - loss 2.2396685777534366 - performance 0.15564017134697763\n",
      "TRAINING - loss 2.2376861425488173 - performance 0.15617900954111769\n",
      "TRAINING - loss 2.2340025748443937 - performance 0.15808344198174706\n",
      "TRAINING - loss 2.231326373603532 - performance 0.15857975843398583\n",
      "TRAINING - loss 2.228543608725333 - performance 0.1595361855257897\n",
      "TRAINING - loss 2.225691438362902 - performance 0.16138023836985774\n",
      "TRAINING - loss 2.2229649563358254 - performance 0.1616068122917438\n",
      "TRAINING - loss 2.221099145665249 - performance 0.16306676187076044\n",
      "TRAINING - loss 2.2187416533772923 - performance 0.16382281971733884\n",
      "TRAINING - loss 2.2148352027217455 - performance 0.16352882372542485\n",
      "TRAINING - loss 2.2123975904477637 - performance 0.16518864882296033\n",
      "TRAINING - loss 2.2082996523704277 - performance 0.16674476726023119\n",
      "TRAINING - loss 2.205874333378764 - performance 0.1673735231747955\n",
      "TRAINING - loss 2.2031605781018473 - performance 0.1673772419876507\n",
      "TRAINING - loss 2.202565628812845 - performance 0.1675235646958012\n",
      "TRAINING - loss 2.1999558343452996 - performance 0.16849486253818383\n",
      "TRAINING - loss 2.1979720859197758 - performance 0.16934612266954877\n",
      "TRAINING - loss 2.1951345397873445 - performance 0.16929755327545382\n",
      "TRAINING - loss 2.192240553249368 - performance 0.17091771340681877\n",
      "TRAINING - loss 2.190578490786182 - performance 0.1709572606848288\n",
      "TRAINING - loss 2.1885367070311776 - performance 0.17184832967568886\n",
      "TRAINING - loss 2.1852245773766272 - performance 0.17341109259700072\n",
      "TRAINING - loss 2.1834753219040848 - performance 0.1745524296675192\n",
      "TRAINING - loss 2.181780925368049 - performance 0.17524426266757556\n",
      "TRAINING - loss 2.1793663382556696 - performance 0.17634970006665185\n",
      "TRAINING - loss 2.1767977592712433 - performance 0.17751575744403392\n",
      "TRAINING - loss 2.1741769474076604 - performance 0.17857902573920442\n",
      "TRAINING - loss 2.1720722100308127 - performance 0.17923349302228703\n",
      "TRAINING - loss 2.170138999871443 - performance 0.17955519281779228\n",
      "TRAINING - loss 2.167905151260016 - performance 0.18026394721055788\n",
      "TRAINING - loss 2.1667408473677225 - performance 0.1813369927465203\n",
      "TRAINING - loss 2.163767546168934 - performance 0.18227263987694675\n",
      "TRAINING - loss 2.1617969615215222 - performance 0.18326730805508396\n",
      "TRAINING - loss 2.159144542875433 - performance 0.18403999259396409\n",
      "TRAINING - loss 2.1561798230450147 - performance 0.1848300309034721\n",
      "TRAINING - loss 2.1537502058657805 - performance 0.18617211212283521\n",
      "TRAINING - loss 2.1518030437622713 - performance 0.18737940712155762\n",
      "TRAINING - loss 2.1505158730281506 - performance 0.1881141182554732\n",
      "TRAINING - loss 2.1481121763741036 - performance 0.1886544653448568\n",
      "TRAINING - loss 2.1455373512151104 - performance 0.18988501916347275\n",
      "TRAINING - loss 2.143827191494934 - performance 0.19058351089985248\n",
      "TRAINING - loss 2.1418357720549617 - performance 0.19150137074665377\n",
      "TRAINING - loss 2.140057750021422 - performance 0.1919139819076337\n",
      "TRAINING - loss 2.137661106026633 - performance 0.19250898297141072\n",
      "TRAINING - loss 2.135631129074566 - performance 0.19331641285956006\n",
      "TRAINING - loss 2.1339700579769665 - performance 0.19425087108013936\n",
      "TRAINING - loss 2.1324922369053394 - performance 0.194672436949709\n",
      "TRAINING - loss 2.13064637003252 - performance 0.19526540214674312\n",
      "TRAINING - loss 2.129531389145795 - performance 0.1953702361976525\n",
      "TRAINING - loss 2.127781976258613 - performance 0.1959005856306242\n",
      "TRAINING - loss 2.126298697443684 - performance 0.1962399662019434\n",
      "TRAINING - loss 2.1246655343920136 - performance 0.19733370365227051\n",
      "TRAINING - loss 2.122639118669915 - performance 0.1982605122585947\n",
      "TRAINING - loss 2.1205055448046055 - performance 0.19872314552087555\n",
      "TRAINING - loss 2.1186880025843307 - performance 0.19940674576723103\n",
      "TRAINING - loss 2.1172144740149466 - performance 0.19971056439942114\n",
      "TRAINING - loss 2.1158408051325956 - performance 0.20052590572652904\n",
      "TRAINING - loss 2.114089122904736 - performance 0.20109601333162416\n",
      "TRAINING - loss 2.11258979358789 - performance 0.20133527401594734\n",
      "TRAINING - loss 2.1108415274005607 - performance 0.20247469066366705\n",
      "TRAINING - loss 2.109432307058993 - performance 0.2031230712257746\n",
      "TRAINING - loss 2.1076455625320785 - performance 0.20384709181807098\n",
      "TRAINING - loss 2.1063019206273728 - performance 0.20389109745813758\n",
      "TRAINING - loss 2.1054375202595343 - performance 0.20435067253898345\n",
      "TRAINING - loss 2.1038177525936863 - performance 0.20497588518997764\n",
      "TRAINING - loss 2.1021261170303776 - performance 0.20512149750029066\n",
      "TRAINING - loss 2.1007423031920607 - performance 0.20534995977473855\n",
      "TRAINING - loss 2.0989727410196295 - performance 0.20591410067037835\n",
      "TRAINING - loss 2.0970551735389744 - performance 0.20632513200763958\n",
      "TRAINING - loss 2.0953676800451837 - performance 0.20675480502166427\n",
      "TRAINING - loss 2.0939823159960884 - performance 0.20695527963960003\n",
      "TRAINING - loss 2.0928197830167234 - performance 0.20726008042604066\n",
      "TRAINING - loss 2.091536831912168 - performance 0.20782711536393936\n",
      "TRAINING - loss 2.089853606152035 - performance 0.20830230826507817\n",
      "TRAINING - loss 2.0880372616607583 - performance 0.20884643721713503\n",
      "TRAINING - loss 2.0878151151590356 - performance 0.20885845224455785\n",
      "TRAINING - loss 2.086704751214174 - performance 0.2092567776517885\n",
      "TRAINING - loss 2.085047906569687 - performance 0.20954494439342924\n",
      "TRAINING - loss 2.08409296218921 - performance 0.20992829007170993\n",
      "TRAINING - loss 2.0827249555322673 - performance 0.21055394460553944\n",
      "TRAINING - loss 2.080871902542673 - performance 0.21096921096921098\n",
      "TRAINING - loss 2.080030302605948 - performance 0.21142535045583766\n",
      "TRAINING - loss 2.078875004452486 - performance 0.21204252014367536\n",
      "TRAINING - loss 2.07726923663643 - performance 0.2126718584751466\n",
      "TRAINING - loss 2.0762640413540043 - performance 0.21309875249976193\n",
      "TRAINING - loss 2.0753238167855868 - performance 0.21358834072257335\n",
      "TRAINING - loss 2.073525965731786 - performance 0.21420895243435192\n",
      "TRAINING - loss 2.0727112690019074 - performance 0.2143551523007129\n",
      "TRAINING - loss 2.0719676429487657 - performance 0.2149802770388038\n",
      "TRAINING - loss 2.071321356438494 - performance 0.21511680756294882\n",
      "TRAINING - loss 2.0697859463289228 - performance 0.21563372669128908\n",
      "TRAINING - loss 2.068536246805231 - performance 0.21600749933041694\n",
      "TRAINING - loss 2.067786294900678 - performance 0.21617555968498364\n",
      "TRAINING - loss 2.0670480796322783 - performance 0.21655995088150162\n",
      "TRAINING - loss 2.066017590754033 - performance 0.21713329275715154\n",
      "TRAINING - loss 2.064924379421014 - performance 0.21773985001292992\n",
      "TRAINING - loss 2.0637158174798405 - performance 0.2180796513118537\n",
      "TRAINING - loss 2.062953112085192 - performance 0.2181594780103381\n",
      "TRAINING - loss 2.062333625210963 - performance 0.21855306276783465\n",
      "TRAINING - loss 2.0616711282638716 - performance 0.2190025831180735\n",
      "TRAINING - loss 2.0607755612109417 - performance 0.21930005784645898\n",
      "TRAINING - loss 2.06050278751413 - performance 0.21934677485452012\n",
      "TRAINING - loss 2.059155535139734 - performance 0.21975855621494186\n",
      "TRAINING - loss 2.057825299984274 - performance 0.22036529312152245\n",
      "Epoch 1 completed. Loss - total: 102835.64991092682 - average: 2.0567129982185364; Performance: 0.22074\n",
      "TRAINING - loss 1.846781849861145 - performance 0.75\n",
      "TRAINING - loss 1.9783505272157123 - performance 0.2376237623762376\n",
      "TRAINING - loss 1.9569352610194268 - performance 0.2599502487562189\n",
      "TRAINING - loss 1.950039411700049 - performance 0.27740863787375414\n",
      "TRAINING - loss 1.9414837193905268 - performance 0.27556109725685785\n",
      "TRAINING - loss 1.941797817776541 - performance 0.26996007984031933\n",
      "TRAINING - loss 1.94042823854183 - performance 0.2637271214642263\n",
      "TRAINING - loss 1.9405251248246762 - performance 0.26783166904422256\n",
      "TRAINING - loss 1.9440339564086495 - performance 0.2671660424469413\n",
      "TRAINING - loss 1.9437660101912793 - performance 0.2705327413984462\n",
      "TRAINING - loss 1.9425597351628703 - performance 0.26973026973026976\n",
      "TRAINING - loss 1.9475512781758182 - performance 0.26725703905540416\n",
      "TRAINING - loss 1.9516633511780699 - performance 0.26769358867610327\n",
      "TRAINING - loss 1.9477826072838011 - performance 0.26940814757878556\n",
      "TRAINING - loss 1.9479847197699427 - performance 0.26873661670235544\n",
      "TRAINING - loss 1.9442358224253746 - performance 0.27165223184543635\n",
      "TRAINING - loss 1.944007568922287 - performance 0.27201748906933165\n",
      "TRAINING - loss 1.941744812651987 - performance 0.2716049382716049\n",
      "TRAINING - loss 1.938517753975978 - performance 0.27193225985563574\n",
      "TRAINING - loss 1.9412291946692068 - performance 0.2711730668069437\n",
      "TRAINING - loss 1.9374227918785014 - performance 0.2739880059970015\n",
      "TRAINING - loss 1.9371909679315704 - performance 0.27367920038077104\n",
      "TRAINING - loss 1.9335682553521398 - performance 0.2743071331213085\n",
      "TRAINING - loss 1.9350758271857689 - performance 0.2737940026075619\n",
      "TRAINING - loss 1.936319103542838 - performance 0.27197001249479386\n",
      "TRAINING - loss 1.9339170747640275 - performance 0.2722910835665734\n",
      "TRAINING - loss 1.9308563743999765 - performance 0.27364475201845445\n",
      "TRAINING - loss 1.9302138837784673 - performance 0.2743428359866716\n",
      "TRAINING - loss 1.9307914677367641 - performance 0.2740092823991432\n",
      "TRAINING - loss 1.9323118595943827 - performance 0.2732678386763185\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/francesco/Repository/Genetics_ANN/Architecture_selection/dense_cnn.ipynb Cell 10\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/francesco/Repository/Genetics_ANN/Architecture_selection/dense_cnn.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mscripts\u001b[39;00m \u001b[39mimport\u001b[39;00m train\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/francesco/Repository/Genetics_ANN/Architecture_selection/dense_cnn.ipynb#X12sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m train_loss, train_acc \u001b[39m=\u001b[39m train\u001b[39m.\u001b[39;49mtrain_model(net, trainloader, criterion, optimizer, num_epochs\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m, device\u001b[39m=\u001b[39;49mdevice)\n",
      "File \u001b[0;32m~/Repository/Genetics_ANN/Architecture_selection/scripts/train.py:44\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, dataloader, loss_fn, optimizer, num_epochs, checkpoint_loc, checkpoint_name, performance, device)\u001b[0m\n\u001b[1;32m     41\u001b[0m loss_meter \u001b[39m=\u001b[39m AverageMeter()\n\u001b[1;32m     42\u001b[0m performance_meter \u001b[39m=\u001b[39m AverageMeter()\n\u001b[0;32m---> 44\u001b[0m train_epoch(model, dataloader, loss_fn, optimizer, loss_meter, performance_meter, performance, device)\n\u001b[1;32m     46\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m completed. Loss - total: \u001b[39m\u001b[39m{\u001b[39;00mloss_meter\u001b[39m.\u001b[39msum\u001b[39m}\u001b[39;00m\u001b[39m - average: \u001b[39m\u001b[39m{\u001b[39;00mloss_meter\u001b[39m.\u001b[39mavg\u001b[39m}\u001b[39;00m\u001b[39m; Performance: \u001b[39m\u001b[39m{\u001b[39;00mperformance_meter\u001b[39m.\u001b[39mavg\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     48\u001b[0m \u001b[39m# produce checkpoint dictionary -- but only if the name and folder of the checkpoint are not None\u001b[39;00m\n",
      "File \u001b[0;32m~/Repository/Genetics_ANN/Architecture_selection/scripts/train.py:14\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, dataloader, loss_fn, optimizer, loss_meter, performance_meter, performance, device)\u001b[0m\n\u001b[1;32m     11\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad() \n\u001b[1;32m     12\u001b[0m \u001b[39m# 2. get the predictions from the current state of the model\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[39m#    this is the forward pass\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m y_hat \u001b[39m=\u001b[39m model(X)\n\u001b[1;32m     15\u001b[0m \u001b[39m# 3. calculate the loss on the current mini-batch\u001b[39;00m\n\u001b[1;32m     16\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(y_hat, y)\n",
      "File \u001b[0;32m~/anaconda3/envs/deep_le/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/home/francesco/Repository/Genetics_ANN/Architecture_selection/dense_cnn.ipynb Cell 10\u001b[0m in \u001b[0;36mDenseNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/francesco/Repository/Genetics_ANN/Architecture_selection/dense_cnn.ipynb#X12sZmlsZQ%3D%3D?line=125'>126</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/francesco/Repository/Genetics_ANN/Architecture_selection/dense_cnn.ipynb#X12sZmlsZQ%3D%3D?line=126'>127</a>\u001b[0m     features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeatures(x)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/francesco/Repository/Genetics_ANN/Architecture_selection/dense_cnn.ipynb#X12sZmlsZQ%3D%3D?line=127'>128</a>\u001b[0m     out \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(features, inplace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/francesco/Repository/Genetics_ANN/Architecture_selection/dense_cnn.ipynb#X12sZmlsZQ%3D%3D?line=128'>129</a>\u001b[0m     out \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39madaptive_avg_pool2d(out, (\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m))\n",
      "File \u001b[0;32m~/anaconda3/envs/deep_le/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/deep_le/lib/python3.9/site-packages/torch/nn/modules/container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    138\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 139\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    140\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/deep_le/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/home/francesco/Repository/Genetics_ANN/Architecture_selection/dense_cnn.ipynb Cell 10\u001b[0m in \u001b[0;36m_DenseBlock.forward\u001b[0;34m(self, init_features)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/francesco/Repository/Genetics_ANN/Architecture_selection/dense_cnn.ipynb#X12sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m features \u001b[39m=\u001b[39m [init_features]\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/francesco/Repository/Genetics_ANN/Architecture_selection/dense_cnn.ipynb#X12sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m \u001b[39mfor\u001b[39;00m name, layer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems():\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/francesco/Repository/Genetics_ANN/Architecture_selection/dense_cnn.ipynb#X12sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m     new_features \u001b[39m=\u001b[39m layer(features)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/francesco/Repository/Genetics_ANN/Architecture_selection/dense_cnn.ipynb#X12sZmlsZQ%3D%3D?line=72'>73</a>\u001b[0m     features\u001b[39m.\u001b[39mappend(new_features)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/francesco/Repository/Genetics_ANN/Architecture_selection/dense_cnn.ipynb#X12sZmlsZQ%3D%3D?line=75'>76</a>\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mcat(features, \u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/deep_le/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/home/francesco/Repository/Genetics_ANN/Architecture_selection/dense_cnn.ipynb Cell 10\u001b[0m in \u001b[0;36m_DenseLayer.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/francesco/Repository/Genetics_ANN/Architecture_selection/dense_cnn.ipynb#X12sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/francesco/Repository/Genetics_ANN/Architecture_selection/dense_cnn.ipynb#X12sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m     prev_features \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/francesco/Repository/Genetics_ANN/Architecture_selection/dense_cnn.ipynb#X12sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m bottleneck_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbn_function(prev_features)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/francesco/Repository/Genetics_ANN/Architecture_selection/dense_cnn.ipynb#X12sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m new_features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu_2(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv3(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mBN_2(bottleneck_output)))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/francesco/Repository/Genetics_ANN/Architecture_selection/dense_cnn.ipynb#X12sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdrop_rate \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "\u001b[1;32m/home/francesco/Repository/Genetics_ANN/Architecture_selection/dense_cnn.ipynb Cell 10\u001b[0m in \u001b[0;36m_DenseLayer.bn_function\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/francesco/Repository/Genetics_ANN/Architecture_selection/dense_cnn.ipynb#X12sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39m# type: (List[Tensor]) -> Tensor\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/francesco/Repository/Genetics_ANN/Architecture_selection/dense_cnn.ipynb#X12sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m concated_features \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat(inputs, \u001b[39m1\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/francesco/Repository/Genetics_ANN/Architecture_selection/dense_cnn.ipynb#X12sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m bottleneck_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu_1(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv1(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mBN_1(concated_features)))  \u001b[39m# noqa: T484\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/francesco/Repository/Genetics_ANN/Architecture_selection/dense_cnn.ipynb#X12sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m \u001b[39mreturn\u001b[39;00m bottleneck_output\n",
      "File \u001b[0;32m~/anaconda3/envs/deep_le/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/deep_le/lib/python3.9/site-packages/torch/nn/modules/batchnorm.py:168\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    161\u001b[0m     bn_training \u001b[39m=\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_mean \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m) \u001b[39mand\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_var \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    163\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    164\u001b[0m \u001b[39mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[1;32m    165\u001b[0m \u001b[39mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \u001b[39mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 168\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mbatch_norm(\n\u001b[1;32m    169\u001b[0m     \u001b[39minput\u001b[39;49m,\n\u001b[1;32m    170\u001b[0m     \u001b[39m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[1;32m    171\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrunning_mean\n\u001b[1;32m    172\u001b[0m     \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrack_running_stats\n\u001b[1;32m    173\u001b[0m     \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    174\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrunning_var \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrack_running_stats \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    175\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[1;32m    176\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias,\n\u001b[1;32m    177\u001b[0m     bn_training,\n\u001b[1;32m    178\u001b[0m     exponential_average_factor,\n\u001b[1;32m    179\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meps,\n\u001b[1;32m    180\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/deep_le/lib/python3.9/site-packages/torch/nn/functional.py:2438\u001b[0m, in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2435\u001b[0m \u001b[39mif\u001b[39;00m training:\n\u001b[1;32m   2436\u001b[0m     _verify_batch_size(\u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize())\n\u001b[0;32m-> 2438\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mbatch_norm(\n\u001b[1;32m   2439\u001b[0m     \u001b[39minput\u001b[39;49m, weight, bias, running_mean, running_var, training, momentum, eps, torch\u001b[39m.\u001b[39;49mbackends\u001b[39m.\u001b[39;49mcudnn\u001b[39m.\u001b[39;49menabled\n\u001b[1;32m   2440\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from scripts import train\n",
    "train_loss, train_acc = train.train_model(net, trainloader, criterion, optimizer, num_epochs=2, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "eval() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/francesco/Repository/Genetics_ANN/Architecture_selection/dense_cnn.ipynb Cell 11\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/francesco/Repository/Genetics_ANN/Architecture_selection/dense_cnn.ipynb#X13sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m net\u001b[39m.\u001b[39;49meval(testloader)\n",
      "\u001b[0;31mTypeError\u001b[0m: eval() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "\n",
    "net.eval(testloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(testloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# print images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "print('GroundTruth: ', ' '.join(f'{classes[labels[j]]:5s}' for j in range(4)))\n",
    "outputs = net(images)\n",
    "_, predicted = torch.max(outputs, 1)\n",
    "\n",
    "print('Predicted: ', ' '.join(f'{classes[predicted[j]]:5s}'\n",
    "                              for j in range(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument weight in method wrapper__native_batch_norm)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/francesco/Repository/Genetics_ANN/Architecture_selection/dense_cnn.ipynb Cell 14\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/francesco/Repository/Genetics_ANN/Architecture_selection/dense_cnn.ipynb#X16sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m images, labels \u001b[39m=\u001b[39m data\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/francesco/Repository/Genetics_ANN/Architecture_selection/dense_cnn.ipynb#X16sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# calculate outputs by running images through the network\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/francesco/Repository/Genetics_ANN/Architecture_selection/dense_cnn.ipynb#X16sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m outputs \u001b[39m=\u001b[39m net(images)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/francesco/Repository/Genetics_ANN/Architecture_selection/dense_cnn.ipynb#X16sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# the class with the highest energy is what we choose as prediction\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/francesco/Repository/Genetics_ANN/Architecture_selection/dense_cnn.ipynb#X16sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m _, predicted \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmax(outputs\u001b[39m.\u001b[39mdata, \u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/deep_le/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/home/francesco/Repository/Genetics_ANN/Architecture_selection/dense_cnn.ipynb Cell 14\u001b[0m in \u001b[0;36mDenseNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/francesco/Repository/Genetics_ANN/Architecture_selection/dense_cnn.ipynb#X16sZmlsZQ%3D%3D?line=125'>126</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/francesco/Repository/Genetics_ANN/Architecture_selection/dense_cnn.ipynb#X16sZmlsZQ%3D%3D?line=126'>127</a>\u001b[0m     features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeatures(x)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/francesco/Repository/Genetics_ANN/Architecture_selection/dense_cnn.ipynb#X16sZmlsZQ%3D%3D?line=127'>128</a>\u001b[0m     out \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(features, inplace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/francesco/Repository/Genetics_ANN/Architecture_selection/dense_cnn.ipynb#X16sZmlsZQ%3D%3D?line=128'>129</a>\u001b[0m     out \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39madaptive_avg_pool2d(out, (\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m))\n",
      "File \u001b[0;32m~/anaconda3/envs/deep_le/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/deep_le/lib/python3.9/site-packages/torch/nn/modules/container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    138\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 139\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    140\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/deep_le/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/home/francesco/Repository/Genetics_ANN/Architecture_selection/dense_cnn.ipynb Cell 14\u001b[0m in \u001b[0;36m_DenseBlock.forward\u001b[0;34m(self, init_features)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/francesco/Repository/Genetics_ANN/Architecture_selection/dense_cnn.ipynb#X16sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m features \u001b[39m=\u001b[39m [init_features]\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/francesco/Repository/Genetics_ANN/Architecture_selection/dense_cnn.ipynb#X16sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m \u001b[39mfor\u001b[39;00m name, layer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems():\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/francesco/Repository/Genetics_ANN/Architecture_selection/dense_cnn.ipynb#X16sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m     new_features \u001b[39m=\u001b[39m layer(features)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/francesco/Repository/Genetics_ANN/Architecture_selection/dense_cnn.ipynb#X16sZmlsZQ%3D%3D?line=72'>73</a>\u001b[0m     features\u001b[39m.\u001b[39mappend(new_features)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/francesco/Repository/Genetics_ANN/Architecture_selection/dense_cnn.ipynb#X16sZmlsZQ%3D%3D?line=75'>76</a>\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mcat(features, \u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/deep_le/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/home/francesco/Repository/Genetics_ANN/Architecture_selection/dense_cnn.ipynb Cell 14\u001b[0m in \u001b[0;36m_DenseLayer.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/francesco/Repository/Genetics_ANN/Architecture_selection/dense_cnn.ipynb#X16sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/francesco/Repository/Genetics_ANN/Architecture_selection/dense_cnn.ipynb#X16sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m     prev_features \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/francesco/Repository/Genetics_ANN/Architecture_selection/dense_cnn.ipynb#X16sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m bottleneck_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbn_function(prev_features)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/francesco/Repository/Genetics_ANN/Architecture_selection/dense_cnn.ipynb#X16sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m new_features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu_2(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv3(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mBN_2(bottleneck_output)))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/francesco/Repository/Genetics_ANN/Architecture_selection/dense_cnn.ipynb#X16sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdrop_rate \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "\u001b[1;32m/home/francesco/Repository/Genetics_ANN/Architecture_selection/dense_cnn.ipynb Cell 14\u001b[0m in \u001b[0;36m_DenseLayer.bn_function\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/francesco/Repository/Genetics_ANN/Architecture_selection/dense_cnn.ipynb#X16sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39m# type: (List[Tensor]) -> Tensor\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/francesco/Repository/Genetics_ANN/Architecture_selection/dense_cnn.ipynb#X16sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m concated_features \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat(inputs, \u001b[39m1\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/francesco/Repository/Genetics_ANN/Architecture_selection/dense_cnn.ipynb#X16sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m bottleneck_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu_1(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv1(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mBN_1(concated_features)))  \u001b[39m# noqa: T484\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/francesco/Repository/Genetics_ANN/Architecture_selection/dense_cnn.ipynb#X16sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m \u001b[39mreturn\u001b[39;00m bottleneck_output\n",
      "File \u001b[0;32m~/anaconda3/envs/deep_le/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/deep_le/lib/python3.9/site-packages/torch/nn/modules/batchnorm.py:168\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    161\u001b[0m     bn_training \u001b[39m=\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_mean \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m) \u001b[39mand\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_var \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    163\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    164\u001b[0m \u001b[39mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[1;32m    165\u001b[0m \u001b[39mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \u001b[39mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 168\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mbatch_norm(\n\u001b[1;32m    169\u001b[0m     \u001b[39minput\u001b[39;49m,\n\u001b[1;32m    170\u001b[0m     \u001b[39m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[1;32m    171\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrunning_mean\n\u001b[1;32m    172\u001b[0m     \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrack_running_stats\n\u001b[1;32m    173\u001b[0m     \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    174\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrunning_var \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrack_running_stats \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    175\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[1;32m    176\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias,\n\u001b[1;32m    177\u001b[0m     bn_training,\n\u001b[1;32m    178\u001b[0m     exponential_average_factor,\n\u001b[1;32m    179\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meps,\n\u001b[1;32m    180\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/deep_le/lib/python3.9/site-packages/torch/nn/functional.py:2438\u001b[0m, in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2435\u001b[0m \u001b[39mif\u001b[39;00m training:\n\u001b[1;32m   2436\u001b[0m     _verify_batch_size(\u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize())\n\u001b[0;32m-> 2438\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mbatch_norm(\n\u001b[1;32m   2439\u001b[0m     \u001b[39minput\u001b[39;49m, weight, bias, running_mean, running_var, training, momentum, eps, torch\u001b[39m.\u001b[39;49mbackends\u001b[39m.\u001b[39;49mcudnn\u001b[39m.\u001b[39;49menabled\n\u001b[1;32m   2440\u001b[0m )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument weight in method wrapper__native_batch_norm)"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        # calculate outputs by running images through the network\n",
    "        outputs = net(images)\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('deep_le')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a142b7d21e8575b7566c168744f24153333bb674c4e2523209192565d5391819"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
