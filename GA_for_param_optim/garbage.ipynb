{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexserra98/miniconda3/envs/myenv/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import grammar as g\n",
    "import utils as t\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import importlib\n",
    "from torch.utils.data import DataLoader\n",
    "from torchsummary import summary\n",
    "from torchvision import datasets, transforms\n",
    "from datetime import datetime \n",
    "import fitness_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gramm = g.Grammar('/home/alexserra98/uni/prog_deep/Genetics_ANN/GA_for_param_optim/cnn.grammar.txt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phenotype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "prova = gramm.initialise('features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_layers = gramm.decode('features', prova)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'layer:conv num-filters:194 filter-shape:2 stride:3 padding:same act:relu bias:False'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Evaluator.__init__() missing 3 required positional arguments: 'fitness_metric', 'batch_size', and 'model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/alexserra98/uni/prog_deep/Genetics_ANN/GA_for_param_optim/garbage.ipynb Cell 8\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/alexserra98/uni/prog_deep/Genetics_ANN/GA_for_param_optim/garbage.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m layer_decoding \u001b[39m=\u001b[39m t\u001b[39m.\u001b[39;49mEvaluator()\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/alexserra98/uni/prog_deep/Genetics_ANN/GA_for_param_optim/garbage.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m l_dict \u001b[39m=\u001b[39m layer_decoding\u001b[39m.\u001b[39mget_layers(feat_layers) \n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/alexserra98/uni/prog_deep/Genetics_ANN/GA_for_param_optim/garbage.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m l_dict\n",
      "\u001b[0;31mTypeError\u001b[0m: Evaluator.__init__() missing 3 required positional arguments: 'fitness_metric', 'batch_size', and 'model'"
     ]
    }
   ],
   "source": [
    "layer_decoding = t.()\n",
    "l_dict = layer_decoding.get_layers(feat_layers) \n",
    "l_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'layer:pool-avg kernel-size:3 stride:1 padding:valid'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(l_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'tmp' from '/home/alexserra98/uni/prog_deep/Genetics_ANN/GA_for_param_optim/tmp.py'>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer_type: pool-avg\n"
     ]
    }
   ],
   "source": [
    "\n",
    "assembler = t.Evaluator()\n",
    "lay = assembler.assemble_network(l_dict,(28,28,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.nn.modules.container.Sequential"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(lay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assembling Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'tmp' from '/home/alexserra98/uni/prog_deep/Genetics_ANN/GA_for_param_optim/tmp.py'>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "gramm = g.Grammar('/home/alexserra98/uni/prog_deep/Genetics_ANN/GA_for_param_optim/cnn.grammar.txt')\n",
    "init_max = {'features' : 5,'classification' : 1, 'learning' : 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### feature module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_module = t.Module('features',1,10,)\n",
    "my_module1 = t.Module('classification',1,2)\n",
    "my_module2 = t.Module('learning',1,2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_module.initialise(gramm,0.2,init_max)\n",
    "my_module1.initialise(gramm,0.2,init_max)\n",
    "my_module2.initialise(gramm,0.2,init_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'features': [{'ge': 0, 'ga': {}}],\n",
       " 'convolution': [{'ge': 0,\n",
       "   'ga': {'num-filters': ('int', 32.0, 256.0, [36]),\n",
       "    'filter-shape': ('int', 2.0, 5.0, [4]),\n",
       "    'stride': ('int', 1.0, 3.0, [2])}}],\n",
       " 'padding': [{'ge': 1, 'ga': {}}],\n",
       " 'activation-function': [{'ge': 2, 'ga': {}}],\n",
       " 'bias': [{'ge': 1, 'ga': {}}]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_module.layers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_layers_decoded = []\n",
    "my_layers_decoded_1 = []\n",
    "my_layers_decoded_2 = []\n",
    "\n",
    "for i in my_module.layers:\n",
    "    my_layers_decoded.append(gramm.decode('features', i))\n",
    "\n",
    "for i in my_module1.layers:\n",
    "    my_layers_decoded_1.append(gramm.decode('classification', i))\n",
    "\n",
    "for i in my_module2.layers:\n",
    "    my_layers_decoded_2.append(gramm.decode('learning', i))\n",
    "\n",
    "pheno_class =' '.join(my_layers_decoded_1)\n",
    "pheno_feat =' '.join(my_layers_decoded)\n",
    "pheno_learn = ' '.join(my_layers_decoded_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'layer:conv num-filters:36 filter-shape:4 stride:2 padding:valid act:sigmoid bias:False layer:conv num-filters:248 filter-shape:4 stride:1 padding:same act:relu bias:False layer:pool-max kernel-size:2 stride:3 padding:same layer:pool-avg kernel-size:4 stride:1 padding:same layer:pool-max kernel-size:2 stride:3 padding:same'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pheno_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = t.Net_encoding(5, 1,3,2, (32,32,3), pheno_feat, pheno_class, pheno_learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict = encoder.get_layers(pheno_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'conv'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_param = encoder.get_learning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer_type: conv\n",
      "layer_type: conv\n",
      "layer_type: conv\n",
      "layer_type: pool-avg\n",
      "layer_type: conv\n",
      "layer_type: fc\n"
     ]
    }
   ],
   "source": [
    "mymodel = encoder.assemble_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 170, 14, 14]          12,750\n",
      "           Sigmoid-2          [-1, 170, 14, 14]               0\n",
      "            Conv2d-3          [-1, 162, 14, 14]         440,802\n",
      "           Sigmoid-4          [-1, 162, 14, 14]               0\n",
      "            Conv2d-5          [-1, 243, 14, 14]         629,856\n",
      "              Tanh-6          [-1, 243, 14, 14]               0\n",
      "         AvgPool2d-7            [-1, 243, 5, 5]               0\n",
      "            Conv2d-8            [-1, 102, 1, 1]         396,678\n",
      "              ReLU-9            [-1, 102, 1, 1]               0\n",
      "           Linear-10                 [-1, 1241]         127,823\n",
      "             Tanh-11                 [-1, 1241]               0\n",
      "           Linear-12                   [-1, 10]          12,420\n",
      "================================================================\n",
      "Total params: 1,620,329\n",
      "Trainable params: 1,620,329\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 1.79\n",
      "Params size (MB): 6.18\n",
      "Estimated Total Size (MB): 7.98\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexserra98/miniconda3/envs/myenv/lib/python3.10/site-packages/torch/nn/modules/conv.py:443: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at  /opt/conda/conda-bld/pytorch_1646755897462/work/aten/src/ATen/native/Convolution.cpp:744.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n"
     ]
    }
   ],
   "source": [
    "summary(mymodel,(3,32,32),device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = encoder.assemble_optimiser(mymodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RMSprop (\n",
       "Parameter Group 0\n",
       "    alpha: 0.99\n",
       "    centered: False\n",
       "    eps: 1e-08\n",
       "    lr: 0.05256362464710246\n",
       "    momentum: 0\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "genNN(\n",
       "  (fe): Sequential(\n",
       "    (0): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (1): Dropout2d(p=0.08870683997615111, inplace=False)\n",
       "    (2): Conv2d(3, 124, kernel_size=(5, 5), stride=(1, 1), padding=same, bias=False)\n",
       "    (3): ReLU()\n",
       "    (4): AvgPool2d(kernel_size=(4, 4), stride=3, padding=0)\n",
       "    (5): Conv2d(124, 229, kernel_size=(2, 2), stride=(2, 2), padding=valid, bias=False)\n",
       "    (6): Sigmoid()\n",
       "    (7): Conv2d(229, 173, kernel_size=(4, 4), stride=(1, 1), padding=same)\n",
       "    (8): Sigmoid()\n",
       "    (9): MaxPool2d(kernel_size=(2, 2), stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): BatchNorm2d(173, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (c): Sequential(\n",
       "    (0): Linear(in_features=173, out_features=446, bias=False)\n",
       "    (1): Tanh()\n",
       "  )\n",
       "  (last): Sequential(\n",
       "    (0): Linear(in_features=446, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mymodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "RANDOM_SEED = 42\n",
    "LEARNING_RATE = 0.001\n",
    "BATCH_SIZE = 32\n",
    "N_EPOCHS = 15\n",
    "\n",
    "IMG_SIZE = 32\n",
    "N_CLASSES = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_accuracy(model, data_loader, device):\n",
    "    '''\n",
    "    Function for computing the accuracy of the predictions over the entire data_loader\n",
    "    '''\n",
    "    \n",
    "    correct_pred = 0 \n",
    "    n = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for X, y_true in data_loader:\n",
    "\n",
    "            X = X.to(device)\n",
    "            y_true = y_true.to(device)\n",
    "\n",
    "            _, y_prob = model(X)\n",
    "            _, predicted_labels = torch.max(y_prob, 1)\n",
    "\n",
    "            n += y_true.size(0)\n",
    "            correct_pred += (predicted_labels == y_true).sum()\n",
    "\n",
    "    return correct_pred.float() / n\n",
    "\n",
    "def plot_losses(train_losses, valid_losses):\n",
    "    '''\n",
    "    Function for plotting training and validation losses\n",
    "    '''\n",
    "    \n",
    "    # temporarily change the style of the plots to seaborn \n",
    "    plt.style.use('seaborn')\n",
    "\n",
    "    train_losses = np.array(train_losses) \n",
    "    valid_losses = np.array(valid_losses)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize = (8, 4.5))\n",
    "\n",
    "    ax.plot(train_losses, color='blue', label='Training loss') \n",
    "    ax.plot(valid_losses, color='red', label='Validation loss')\n",
    "    ax.set(title=\"Loss over epochs\", \n",
    "            xlabel='Epoch',\n",
    "            ylabel='Loss') \n",
    "    ax.legend()\n",
    "    fig.show()\n",
    "    \n",
    "    # change the plot style to default\n",
    "    plt.style.use('default')\n",
    "    \n",
    "def train(train_loader, model, criterion, optimizer, device):\n",
    "    '''\n",
    "    Function for the training step of the training loop\n",
    "    '''\n",
    "\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    \n",
    "    for X, y_true in train_loader:\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        X = X.to(device)\n",
    "        y_true = y_true.to(device)\n",
    "    \n",
    "        # Forward pass\n",
    "        y_hat, _ = model(X) \n",
    "        loss = criterion(y_hat, y_true) \n",
    "        running_loss += loss.item() * X.size(0)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    return model, optimizer, epoch_loss\n",
    "\n",
    "def validate(valid_loader, model, criterion, device):\n",
    "    '''\n",
    "    Function for the validation step of the training loop\n",
    "    '''\n",
    "   \n",
    "    model.eval()\n",
    "    running_loss = 0\n",
    "    \n",
    "    for X, y_true in valid_loader:\n",
    "    \n",
    "        X = X.to(device)\n",
    "        y_true = y_true.to(device)\n",
    "\n",
    "        # Forward pass and record loss\n",
    "        y_hat, _ = model(X) \n",
    "        loss = criterion(y_hat, y_true) \n",
    "        running_loss += loss.item() * X.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(valid_loader.dataset)\n",
    "        \n",
    "    return model, epoch_loss\n",
    "\n",
    "def training_loop(model, criterion, optimizer, train_loader, valid_loader, epochs, device, print_every=1):\n",
    "    '''\n",
    "    Function defining the entire training loop\n",
    "    '''\n",
    "    \n",
    "    # set objects for storing metrics\n",
    "    best_loss = 1e10\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    " \n",
    "    # Train model\n",
    "    for epoch in range(0, epochs):\n",
    "\n",
    "        # training\n",
    "        model, optimizer, train_loss = train(train_loader, model, criterion, optimizer, device)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        # validation\n",
    "        with torch.no_grad():\n",
    "            model, valid_loss = validate(valid_loader, model, criterion, device)\n",
    "            valid_losses.append(valid_loss)\n",
    "\n",
    "        if epoch % print_every == (print_every - 1):\n",
    "            \n",
    "            train_acc = get_accuracy(model, train_loader, device=device)\n",
    "            valid_acc = get_accuracy(model, valid_loader, device=device)\n",
    "                \n",
    "            print(f'{datetime.now().time().replace(microsecond=0)} --- '\n",
    "                  f'Epoch: {epoch}\\t'\n",
    "                  f'Train loss: {train_loss:.4f}\\t'\n",
    "                  f'Valid loss: {valid_loss:.4f}\\t'\n",
    "                  f'Train accuracy: {100 * train_acc:.2f}\\t'\n",
    "                  f'Valid accuracy: {100 * valid_acc:.2f}')\n",
    "\n",
    "    plot_losses(train_losses, valid_losses)\n",
    "    \n",
    "    return model, optimizer, (train_losses, valid_losses)\n",
    "\n",
    "# define transforms\n",
    "# transforms.ToTensor() automatically scales the images to [0,1] range\n",
    "transforms = transforms.Compose([transforms.Resize((32, 32)),\n",
    "                                 transforms.ToTensor()])\n",
    "\n",
    "# download and create datasets\n",
    "train_dataset = datasets.CIFAR10(root='cifar_data', \n",
    "                               train=True, \n",
    "                               transform=transforms,\n",
    "                               download=True)\n",
    "\n",
    "valid_dataset = datasets.CIFAR10(root='cifar_data', \n",
    "                               train=False, \n",
    "                               transform=transforms)\n",
    "\n",
    "# define the data loaders\n",
    "train_loader = DataLoader(dataset=train_dataset, \n",
    "                          batch_size=BATCH_SIZE, \n",
    "                          shuffle=True)\n",
    "\n",
    "valid_loader = DataLoader(dataset=valid_dataset, \n",
    "                          batch_size=BATCH_SIZE, \n",
    "                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mymodel\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "09:47:49 --- Epoch: 0\tTrain loss: 1.6836\tValid loss: 1.4797\tTrain accuracy: 46.78\tValid accuracy: 46.45\n",
      "09:49:09 --- Epoch: 1\tTrain loss: 1.4204\tValid loss: 1.3766\tTrain accuracy: 52.66\tValid accuracy: 51.32\n",
      "09:50:37 --- Epoch: 2\tTrain loss: 1.3105\tValid loss: 1.2896\tTrain accuracy: 56.75\tValid accuracy: 53.78\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/alexserra98/uni/prog_deep/Genetics_ANN/GA_for_param_optim/garbage.ipynb Cell 27\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/alexserra98/uni/prog_deep/Genetics_ANN/GA_for_param_optim/garbage.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m DEVICE \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/alexserra98/uni/prog_deep/Genetics_ANN/GA_for_param_optim/garbage.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m model, optimizer, _ \u001b[39m=\u001b[39m training_loop(model, criterion, optimizer, train_loader, valid_loader, N_EPOCHS, DEVICE)\n",
      "\u001b[1;32m/home/alexserra98/uni/prog_deep/Genetics_ANN/GA_for_param_optim/garbage.ipynb Cell 27\u001b[0m in \u001b[0;36mtraining_loop\u001b[0;34m(model, criterion, optimizer, train_loader, valid_loader, epochs, device, print_every)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu/home/alexserra98/uni/prog_deep/Genetics_ANN/GA_for_param_optim/garbage.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=106'>107</a>\u001b[0m \u001b[39m# Train model\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu/home/alexserra98/uni/prog_deep/Genetics_ANN/GA_for_param_optim/garbage.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=107'>108</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, epochs):\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu/home/alexserra98/uni/prog_deep/Genetics_ANN/GA_for_param_optim/garbage.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=108'>109</a>\u001b[0m \n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu/home/alexserra98/uni/prog_deep/Genetics_ANN/GA_for_param_optim/garbage.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=109'>110</a>\u001b[0m     \u001b[39m# training\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/alexserra98/uni/prog_deep/Genetics_ANN/GA_for_param_optim/garbage.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=110'>111</a>\u001b[0m     model, optimizer, train_loss \u001b[39m=\u001b[39m train(train_loader, model, criterion, optimizer, device)\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu/home/alexserra98/uni/prog_deep/Genetics_ANN/GA_for_param_optim/garbage.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=111'>112</a>\u001b[0m     train_losses\u001b[39m.\u001b[39mappend(train_loss)\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu/home/alexserra98/uni/prog_deep/Genetics_ANN/GA_for_param_optim/garbage.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=113'>114</a>\u001b[0m     \u001b[39m# validation\u001b[39;00m\n",
      "\u001b[1;32m/home/alexserra98/uni/prog_deep/Genetics_ANN/GA_for_param_optim/garbage.ipynb Cell 27\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_loader, model, criterion, optimizer, device)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/alexserra98/uni/prog_deep/Genetics_ANN/GA_for_param_optim/garbage.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=65'>66</a>\u001b[0m     running_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem() \u001b[39m*\u001b[39m X\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/alexserra98/uni/prog_deep/Genetics_ANN/GA_for_param_optim/garbage.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=67'>68</a>\u001b[0m     \u001b[39m# Backward pass\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/alexserra98/uni/prog_deep/Genetics_ANN/GA_for_param_optim/garbage.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=68'>69</a>\u001b[0m     loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/alexserra98/uni/prog_deep/Genetics_ANN/GA_for_param_optim/garbage.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=69'>70</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/alexserra98/uni/prog_deep/Genetics_ANN/GA_for_param_optim/garbage.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=71'>72</a>\u001b[0m epoch_loss \u001b[39m=\u001b[39m running_loss \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(train_loader\u001b[39m.\u001b[39mdataset)\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.10/site-packages/torch/_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    355\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    356\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    357\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    361\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    362\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> 363\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.10/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "DEVICE = 'cpu'\n",
    "model, optimizer, _ = training_loop(model, criterion, optimizer, train_loader, valid_loader, N_EPOCHS, DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'tmp' from '/home/alexserra98/uni/prog_deep/Genetics_ANN/GA_for_param_optim/tmp.py'>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'fitness_metrics' from '/home/alexserra98/uni/prog_deep/Genetics_ANN/GA_for_param_optim/fitness_metrics.py'>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(fitness_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval = t.Evaluator(fitness_metrics.accuracy,1,model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: 100%|██████████| 1563/1563 [00:58<00:00, 26.82it/s]\n",
      "evaluating: 100%|██████████| 313/313 [00:04<00:00, 72.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16:21:05 --- Epoch: 0\tTrain loss: 2.3032\tValid loss: 2.3028\tTrain accuracy: 10.00\tValid accuracy: 10.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:  70%|███████   | 1096/1563 [00:40<00:17, 26.80it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/alexserra98/uni/prog_deep/Genetics_ANN/GA_for_param_optim/garbage.ipynb Cell 34\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/alexserra98/uni/prog_deep/Genetics_ANN/GA_for_param_optim/garbage.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39meval\u001b[39;49m\u001b[39m.\u001b[39;49mtrain(\u001b[39m0\u001b[39;49m,\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,train_loader,valid_loader, optimizer, epochs\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m)\n",
      "File \u001b[0;32m~/uni/prog_deep/Genetics_ANN/GA_for_param_optim/tmp.py:845\u001b[0m, in \u001b[0;36mEvaluator.train\u001b[0;34m(self, load_prev_weights, weights_save_path, trainloader, validloader, optimizer, batch_size, epochs, inspected, print_every)\u001b[0m\n\u001b[1;32m    843\u001b[0m y_hat, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel(X) \n\u001b[1;32m    844\u001b[0m loss \u001b[39m=\u001b[39m criterion(y_hat, y_true) \n\u001b[0;32m--> 845\u001b[0m running_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39;49mitem() \u001b[39m*\u001b[39m X\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m)\n\u001b[1;32m    847\u001b[0m \u001b[39m# calculate outputs by running images through the network\u001b[39;00m\n\u001b[1;32m    848\u001b[0m \u001b[39m# Backward pass\u001b[39;00m\n\u001b[1;32m    849\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "eval.train(0,\"\",train_loader,valid_loader, optimizer, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = os.path.join(folder_save, \"model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tmp.genNN"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'tmp' from '/home/alexserra98/uni/prog_deep/Genetics_ANN/GA_for_param_optim/tmp.py'>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = t.Individual([('features',1,10),('classification',1,2)],['learning'],0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.macro: [{'learning': [{'ge': 2, 'ga': {'batch_size': ('int', 50.0, 500.0, [391])}}], 'adam': [{'ge': 0, 'ga': {'lr': ('float', 0.0001, 0.1, [0.06883574665862617]), 'beta1': ('float', 0.5, 1.0, [0.5806125431159568]), 'beta2': ('float', 0.5, 1.0, [0.7261197681079284]), 'decay': ('float', 1e-06, 0.001, [0.0009131089004599922])}}], 'early-stop': [{'ge': 0, 'ga': {'early_stop': ('int', 5.0, 20.0, [14])}}]}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tmp.Individual at 0x7f0b95f89cf0>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind.initialise(gramm,0.2,init_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'layer:batch-norm layer:conv num-filters:71 filter-shape:5 stride:2 padding:same act:relu bias:False layer:conv num-filters:243 filter-shape:5 stride:2 padding:same act:tanh bias:False layer:batch-norm layer:conv num-filters:63 filter-shape:3 stride:2 padding:same act:tanh bias:False layer:fc act:sigmoid num-units:584 bias:False learning:adam lr:0.06883574665862617 beta1:0.5806125431159568 beta2:0.7261197681079284 decay:0.0009131089004599922 early_stop:14 batch_size:391 epochs:400'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind.decode(gramm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'learning': [{'ge': 2, 'ga': {'batch_size': ('int', 50.0, 500.0, [391])}}],\n",
       "  'adam': [{'ge': 0,\n",
       "    'ga': {'lr': ('float', 0.0001, 0.1, [0.06883574665862617]),\n",
       "     'beta1': ('float', 0.5, 1.0, [0.5806125431159568]),\n",
       "     'beta2': ('float', 0.5, 1.0, [0.7261197681079284]),\n",
       "     'decay': ('float', 1e-06, 0.001, [0.0009131089004599922])}}],\n",
       "  'early-stop': [{'ge': 0, 'ga': {'early_stop': ('int', 5.0, 20.0, [14])}}]}]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind.macro"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('myenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "273c48406751af6277a0bdb291ee674620b66e29f007a13179c473c693907be6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
